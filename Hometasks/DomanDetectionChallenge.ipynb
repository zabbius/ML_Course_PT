{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T17:33:07.092633643Z",
     "start_time": "2026-02-05T17:32:59.106808139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_data = pd.read_csv(\"./data/kaggle/input/dga-domain-detection-challenge-i/train.csv.gz\")\n",
    "full_train, full_test = train_test_split(full_data, test_size=0.25, random_state=42)\n",
    "\n",
    "\n"
   ],
   "id": "ffeca477fca3d917",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T17:36:37.133084835Z",
     "start_time": "2026-02-05T17:36:35.223453758Z"
    }
   },
   "cell_type": "code",
   "source": "max(len(str(d).split('.')[0]) for d in full_data[\"domain\"].values)",
   "id": "c04025730805bd1f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T02:06:26.485819511Z",
     "start_time": "2026-02-05T02:06:26.454372439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from math import log2\n",
    "\n",
    "vowels = set(\"aeiou\")\n",
    "consonants = set(\"bcdfghjklmnpqrstvwxz\")\n",
    "\n",
    "forbidden_bigrams = {\n",
    "    'qf', 'qj', 'qk', 'qv', 'qz', 'qy',\n",
    "    'fj', 'vj', 'dj', 'wj', 'tj', 'hj', 'bj', 'cj', 'gj', 'kj', 'lj', 'mj', 'nj', 'pj', 'rj', 'sj', 'xj', 'zj',\n",
    "    'bv', 'dv', 'jv', 'mv', 'pv', 'sv', 'tv', 'vv', 'cv', 'fv', 'gv', 'hv', 'kv', 'lv', 'nv', 'qv', 'rv', 'wv', 'xv',\n",
    "    'zv',\n",
    "    'kk', 'vv', 'ww', 'xx', 'jj',\n",
    "    'ckq', 'jw', 'qj', 'vf', 'vk', 'vp', 'vw', 'vz', 'wk', 'wq', 'wu', 'wz', 'xq', 'yw', 'yz',\n",
    "    'fx', 'gq', 'gx', 'hq', 'hx', 'jq', 'jx', 'jz', 'kq', 'kx', 'pq', 'px', 'qa', 'qe', 'qg', 'qh', 'qi', 'qm', 'qn',\n",
    "    'qo', 'qr', 'qs', 'qt', 'qu', 'qx',\n",
    "    'qz', 'sx', 'vx', 'wx', 'xj', 'xr', 'xz', 'zq', 'zx'\n",
    "}\n",
    "\n",
    "common_bigrams = {\n",
    "    'co', 'my', 'in', 're', 'go', 'to', 'on', 'we', 'hi', 'st',\n",
    "    'te', 'ma', 'no', 'ne', 'ha', 'he', 'ho', 'do', 'sh', 'me',\n",
    "    'er', 'ly', 'ng', 'ed', 'es', 'al', 'or', 'ty', 'ra', 'li',\n",
    "    'an', 'ar', 'en', 'el', 'ch', 'ic', 'ck', 'rd', 'ss', 'tt',\n",
    "    'fy', 'io', 'ti', 'ai', 'ro', 'mo', 've', 'ea', 'oo', 'ou',\n",
    "    'ei', 'ie', 'ba', 'be', 'ca', 'ce', 'da', 'de', 'fa', 'fe',\n",
    "    'ga', 'ge', 'at', 'et', 'it'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def shannon_entropy(s):\n",
    "    probs = [s.count(c) / len(s) for c in set(s)]\n",
    "    return -sum(p * log2(p) for p in probs)\n",
    "\n",
    "\n",
    "def get_ngrams(s, n=2):\n",
    "    return [s[i:i + n] for i in range(len(s) - n + 1)]\n",
    "\n",
    "\n",
    "def extract_features(domain):\n",
    "    domain = str(domain).split('.')[0]\n",
    "\n",
    "    entropy = shannon_entropy(domain)\n",
    "    length = len(domain)\n",
    "\n",
    "    digit_count = sum(c.isdigit() for c in domain)\n",
    "    special_count = sum(not c.isalnum() for c in domain)\n",
    "\n",
    "    vowel_count = sum(c in vowels for c in domain)\n",
    "    consonant_count = sum(c in consonants for c in domain)\n",
    "\n",
    "    digit_sequences = re.findall(r'\\d+', domain)\n",
    "    vowel_sequences = re.findall(f'[{vowels}]+', domain)\n",
    "    consonant_sequences = re.findall(f'[{consonants}]+', domain)\n",
    "\n",
    "    max_digits = max([len(seq) for seq in digit_sequences]) if digit_sequences else 0\n",
    "    max_vowels = max([len(seq) for seq in vowel_sequences]) if vowel_sequences else 0\n",
    "    max_consonants = max([len(seq) for seq in consonant_sequences]) if consonant_sequences else 0\n",
    "\n",
    "    bigrams = get_ngrams(domain, 2)\n",
    "    bigram_entropy = shannon_entropy(bigrams)\n",
    "    uniq_bigram_count = len(set(bigrams))\n",
    "\n",
    "    forbidden_bigrams_count = sum(b in forbidden_bigrams for b in bigrams)\n",
    "    common_bigrams_count = sum(b in common_bigrams for b in bigrams)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"length\": length,\n",
    "        \"entropy\": entropy,\n",
    "\n",
    "        \"digit_ratio\": digit_count / length,\n",
    "        \"special_ratio\": special_count / length,\n",
    "        \"vowel_ratio\": vowel_count / length,\n",
    "        \"consonant_ratio\": consonant_count / length,\n",
    "\n",
    "        \"max_digits\": max_digits,\n",
    "        \"max_vowels\": max_vowels,\n",
    "        \"max_consonants\": max_consonants,\n",
    "\n",
    "        \"bigram_entropy\": bigram_entropy,\n",
    "        \"uniq_bigram_count\": uniq_bigram_count,\n",
    "\n",
    "        \"forbidden_bigrams_count\": forbidden_bigrams_count,\n",
    "        \"common_bigrams_count\": common_bigrams_count,\n",
    "    }"
   ],
   "id": "5134f8729468084a",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T02:09:14.562238893Z",
     "start_time": "2026-02-05T02:06:29.579988483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train = full_train.sample(frac=0.1, random_state=42)\n",
    "test = full_test.sample(frac=0.1, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame([extract_features(str(d)) for d in tqdm(train[\"domain\"], desc=\"Extracting train features\")])\n",
    "y_train = train[\"label\"].values\n",
    "\n",
    "X_test = pd.DataFrame([extract_features(str(d)) for d in tqdm(test[\"domain\"], desc=\"Extracting test features\")])\n",
    "y_test = test[\"label\"].values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n"
   ],
   "id": "b174d8c40bbeb6a4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features: 100%|██████████| 6644921/6644921 [01:53<00:00, 58759.48it/s]\n",
      "Extracting test features: 100%|██████████| 2214974/2214974 [00:37<00:00, 59254.81it/s]\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T02:09:29.480637376Z",
     "start_time": "2026-02-05T02:09:18.968748507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "lr_score = fbeta_score(y_test, y_test_pred, beta=0.5)\n",
    "print(f\"LogisticRegression score: {lr_score}\")\n"
   ],
   "id": "c0abd1b4c3fd610b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression score: 0.8703194026684563\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T02:15:06.993584386Z",
     "start_time": "2026-02-05T02:09:43.484784282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    class_weight='balanced', random_state=42, n_jobs=-1,\n",
    "    n_estimators=150,\n",
    "    max_depth=20,\n",
    "    max_features='sqrt',\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "rf_score = fbeta_score(y_test, y_test_pred, beta=0.5)\n",
    "print(f\"RandomForestClassifier score: {rf_score}\")\n"
   ],
   "id": "27c9795f6c7d76d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier score: 0.9097261764635975\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=8)\n",
    "param_grid = {\n",
    "    'n_estimators': [150],\n",
    "    'max_depth': [10, 20],\n",
    "    'max_features': ['sqrt', 'log2', 0.3, 0.5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "scorer = make_scorer(fbeta_score, beta=0.5, greater_is_better=True)\n",
    "\n",
    "search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=scorer,\n",
    "    n_jobs=2,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(\"Best score:\", search.best_score_)\n",
    "\n",
    "final_model = search.best_estimator_\n",
    "\n",
    "y_test_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "rf_score = fbeta_score(y_test, y_test_pred, beta=0.5)\n",
    "print(f\"RandomForestClassifier score: {rf_score}\")\n"
   ],
   "id": "7f14545fe66178dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T01:57:57.667597403Z",
     "start_time": "2026-02-05T01:57:57.631771440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance)"
   ],
   "id": "f6bb1dc4342c88ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    feature  importance\n",
      "11  forbidden_bigrams_count    0.192985\n",
      "8            max_consonants    0.178362\n",
      "12     common_bigrams_count    0.134335\n",
      "9            bigram_entropy    0.081980\n",
      "4               vowel_ratio    0.076596\n",
      "1                   entropy    0.065009\n",
      "0                    length    0.060156\n",
      "5           consonant_ratio    0.056084\n",
      "10        uniq_bigram_count    0.054880\n",
      "3             special_ratio    0.041534\n",
      "7                max_vowels    0.029067\n",
      "2               digit_ratio    0.016650\n",
      "6                max_digits    0.012362\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T02:29:08.200809868Z",
     "start_time": "2026-02-05T02:26:44.406323636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_test = pd.read_csv(\"./data/kaggle/input/dga-domain-detection-challenge-i/test.csv.gz\")\n",
    "\n",
    "test_features = pd.DataFrame(\n",
    "    [extract_features(str(d)) for d in tqdm(data_test[\"domain\"], desc=\"Extracting test features\")])\n",
    "test_features_scaled = scaler.fit_transform(test_features)\n",
    "\n"
   ],
   "id": "887b83757efaa651",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test features: 100%|██████████| 7594197/7594197 [02:09<00:00, 58850.91it/s]\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T02:29:42.198375087Z",
     "start_time": "2026-02-05T02:29:12.839687858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_test[\"label\"] = rf_model.predict(test_features_scaled)\n",
    "data_test[[\"id\", \"label\"]].to_csv(\"submission_rf3.csv\", index=False)\n"
   ],
   "id": "5262462bf830de21",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "59c93f16a802b7a4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
