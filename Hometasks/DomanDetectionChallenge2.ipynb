{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from keras.src.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"./data/kaggle/input/dga-domain-detection-challenge-i/train.csv.gz\")\n",
    "\n",
    "print(f'Full data shape: {data.shape}')\n",
    "\n",
    "data = data.sample(frac=0.3, random_state=42)\n",
    "\n",
    "print(f'Data shape: {data.shape}')\n",
    "\n",
    "def preprocess_domain(domain):\n",
    "    return str(domain).split('.')[0]\n",
    "\n",
    "\n",
    "data[\"domain\"] = data[\"domain\"].apply(preprocess_domain)\n"
   ],
   "id": "ffeca477fca3d917",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 64  # Максимальная длина домена\n",
    "MAX_WORDS = 64  # Максимальный размер словаря\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, char_level=True, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['domain'].values)\n",
    "\n",
    "vocab_size = min(MAX_WORDS, len(tokenizer.word_index) + 1)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data['domain'].values)\n",
    "\n",
    "# Считаем OOV\n",
    "total_tokens = 0\n",
    "oov_tokens = 0\n",
    "\n",
    "for seq in sequences:\n",
    "    total_tokens += len(seq)\n",
    "    oov_tokens += seq.count(1)  # <OOV> имеет индекс 1\n",
    "\n",
    "oov_percentage = (oov_tokens / total_tokens) * 100\n",
    "print(f\"vocab_size={MAX_WORDS}: {total_tokens} tokens, {oov_percentage:.1f}% OOV\")\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y = data['label'].values\n",
    "\n"
   ],
   "id": "724e6f9dcb394d58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42, stratify=y_train)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n"
   ],
   "id": "e4dfa90666679b32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from keras.src.backend import epsilon\n",
    "\n",
    "# Простая реализация, которая работает без ошибок\n",
    "def simple_fbeta_score(beta=0.5):\n",
    "    \"\"\"Простая F-beta метрика без внутренних tf.Variable\"\"\"\n",
    "    beta_squared = beta ** 2\n",
    "\n",
    "    def fbeta(y_true, y_pred):\n",
    "        # Для sparse categorical (y_true: int, y_pred: probabilities)\n",
    "        y_pred_class = tf.argmax(y_pred, axis=-1)\n",
    "        y_true_class = tf.cast(y_true, tf.int64)\n",
    "\n",
    "        # Создаем confusion matrix\n",
    "        tp = tf.reduce_sum(tf.cast((y_true_class == 1) & (y_pred_class == 1), tf.float32))\n",
    "        fp = tf.reduce_sum(tf.cast((y_true_class == 0) & (y_pred_class == 1), tf.float32))\n",
    "        fn = tf.reduce_sum(tf.cast((y_true_class == 1) & (y_pred_class == 0), tf.float32))\n",
    "\n",
    "        # Вычисляем precision и recall\n",
    "        precision = tp / (tp + fp + epsilon())\n",
    "        recall = tp / (tp + fn + epsilon())\n",
    "\n",
    "        # F-beta score\n",
    "        fbeta_value = (1 + beta_squared) * (precision * recall) / (\n",
    "            beta_squared * precision + recall + epsilon())\n",
    "\n",
    "        return fbeta_value\n",
    "\n",
    "    # Даем метрике имя для отображения\n",
    "    fbeta.__name__ = f'f_beta_{beta}_score'\n",
    "    return fbeta\n"
   ],
   "id": "a4b39b20868d09ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras import layers, models\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(MAX_LEN,)),\n",
    "    layers.Embedding(input_dim=vocab_size,output_dim=128),\n",
    "\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Bidirectional(layers.LSTM(32)),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Dense(2, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[simple_fbeta_score()]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ],
   "id": "b3bde5c056566b12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras import layers, models\n",
    "from keras.src.legacy.layers import ThresholdedReLU\n",
    "\n",
    "inputs = layers.Input(shape=(MAX_LEN,))\n",
    "\n",
    "x = layers.Embedding(input_dim=vocab_size,output_dim=128)(inputs)\n",
    "\n",
    "x1 = layers.Conv1D(filters=256, kernel_size=2, activation='relu', padding='same')(x)\n",
    "x1 = ThresholdedReLU(1e-5)(x1)\n",
    "\n",
    "x2 = layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x2 = ThresholdedReLU(1e-5)(x2)\n",
    "\n",
    "x = layers.Concatenate()([x1, x2])\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = ThresholdedReLU(1e-5)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[simple_fbeta_score()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "e623cf54c7628005",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.src.legacy.layers import ThresholdedReLU\n",
    "from keras import layers, models\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(MAX_LEN,)),\n",
    "    layers.Embedding(input_dim=vocab_size,output_dim=128),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, padding=\"same\", strides=1),\n",
    "    ThresholdedReLU(1e-6),\n",
    "    layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n",
    "    layers.Conv1D(filters=128, kernel_size=2, padding=\"same\", strides=1),\n",
    "    ThresholdedReLU(1e-6),\n",
    "    layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64),\n",
    "    ThresholdedReLU(1e-6),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(2, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[simple_fbeta_score()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "357aab26dbb9df58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.src.legacy.layers import ThresholdedReLU\n",
    "from keras import layers, models\n",
    "\n",
    "vocab_size = min(MAX_WORDS, len(tokenizer.word_index) + 1)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(MAX_LEN,)),\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=128),\n",
    "    layers.Conv1D(filters=128, kernel_size=6, activation='relu', padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(2, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[simple_fbeta_score()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "ef631a48e956ff8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Параметры обучения\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Обучение модели\n",
    "print(\"Начинаем обучение модели...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n"
   ],
   "id": "fa1e816b16370a95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Визуализация истории обучения\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # График точности\n",
    "    ax1.plot(history.history['f_beta_05_score'], label='Точность на обучении', linewidth=2)\n",
    "    ax1.plot(history.history['val_f_beta_05_score'], label='Точность на валидации', linewidth=2)\n",
    "    ax1.set_title('Точность модели', fontsize=14)\n",
    "    ax1.set_xlabel('Эпоха', fontsize=12)\n",
    "    ax1.set_ylabel('Точность', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # График потерь\n",
    "    ax2.plot(history.history['loss'], label='Потери на обучении', linewidth=2)\n",
    "    ax2.plot(history.history['val_loss'], label='Потери на валидации', linewidth=2)\n",
    "    ax2.set_title('Потери модели', fontsize=14)\n",
    "    ax2.set_xlabel('Эпоха', fontsize=12)\n",
    "    ax2.set_ylabel('Потери', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Визуализируем историю обучения\n",
    "plot_training_history(history)\n"
   ],
   "id": "d919895e291b79d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Точность на тестовой выборке: {test_accuracy:.4f}\")\n",
    "print(f\"Потери на тестовой выборке: {test_loss:.4f}\")"
   ],
   "id": "7028b56c17dc83e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.save('dga_detector_cnn_lstm.keras')\n",
    "\n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "id": "6d6382dbbe585ac5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_test = pd.read_csv(\"./data/kaggle/input/dga-domain-detection-challenge-i/test.csv.gz\")\n",
    "\n",
    "print(f'Test shape: {data_test.shape}')\n",
    "data_test[\"domain\"] = data_test[\"domain\"].apply(preprocess_domain)\n"
   ],
   "id": "402581e1d93b0cd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(data_test['domain'].values)\n",
    "print(f'test_sequences len: {len(test_sequences)}')\n",
    "\n",
    "X_predict = pad_sequences(test_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "print(f'X_predict len: {len(X_predict)}')\n",
    "\n",
    "y_pred_proba = model.predict(X_predict)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "print(f'y_pred len: {len(y_pred)}')\n",
    "\n"
   ],
   "id": "afaf129054d3e5c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_test[\"label\"] = y_pred\n",
    "data_test[[\"id\", \"label\"]].to_csv(\"submission_cnnlstm2.csv\", index=False)\n"
   ],
   "id": "acdfdb93892a23f6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
